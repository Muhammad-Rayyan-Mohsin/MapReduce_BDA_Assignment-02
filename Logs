Project Update Log - March 2024

Data Preprocessing and TF-IDF Vector Generation:

In March 2024, significant progress was made on our project aimed at analyzing and extracting insights from a massive dataset of textual information. The first phase of our work involved meticulous data preprocessing, primarily utilizing the Natural Language Toolkit (NLTK). This step was crucial as it laid the foundation for subsequent analyses by ensuring the text data was clean, standardized, and ready for further processing.

Following successful data preprocessing, we moved on to the development of code for generating TF-IDF (Term Frequency-Inverse Document Frequency) vectors for the documents. TF-IDF is a fundamental technique in text mining and information retrieval, allowing us to quantify the importance of words within documents relative to the entire corpus. By implementing TF-IDF, we aimed to capture the significance of terms while mitigating the impact of common words that appear frequently across documents.

Hadoop Clusters on Microsoft Azure VMs:

One of the key challenges we encountered during the project was handling the sheer volume of data. To overcome this challenge, we leveraged the scalability and parallel processing capabilities of Hadoop, deploying local clusters on Microsoft Azure Virtual Machines (VMs). This strategic decision allowed us to distribute the data processing tasks efficiently across multiple nodes, enabling faster computations and efficient resource utilization.

Extraction and Cleaning of SECTION_TEXT and ARTICLE_ID Columns:

With the infrastructure in place, our focus shifted towards extracting pertinent information from our large dataset. Specifically, we targeted the SECTION_TEXT and ARTICLE_ID columns, recognizing their importance in subsequent analyses. Through a series of extraction and cleaning procedures, we meticulously curated the relevant data, ensuring accuracy and consistency throughout the process.

Input Preparation for Mapper and Reducer Files:

To facilitate further data processing within the Hadoop framework, we prepared the extracted and cleaned data for ingestion into mapper and reducer files. This involved formatting the data into a structured format compatible with Hadoop's MapReduce paradigm, effectively streamlining the subsequent stages of analysis.

Next Steps:

With the groundwork laid in data preprocessing, TF-IDF vector generation, infrastructure setup, and data preparation, we are poised to embark on the next phase of our project. Moving forward, our objectives include:

    Implementing MapReduce tasks to process the input data efficiently within the Hadoop clusters.
    Performing advanced analyses and computations leveraging the TF-IDF vectors to extract meaningful insights from the dataset.
    Iteratively refining our methodologies and algorithms to optimize performance and accuracy.
    Documenting and presenting our findings comprehensively to stakeholders, highlighting key insights and implications derived from the analysis.

As we progress through these stages, we remain committed to delivering a robust and insightful analysis that contributes valuable knowledge and understanding to our domain of interest.
